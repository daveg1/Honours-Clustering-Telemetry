{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "telemetry = pd.read_csv('./telemetry_cleaned.csv')\n",
    "X = telemetry[['Easting', 'Northing', 'WaterDepth', 'Roll', 'Pitch', 'Heading']].head(10619)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_clusters(db):\n",
    "\treturn len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search function\n",
    "\n",
    "This function accepts an input dataset and a list of options. The function will iterate through each combination of these options in order to find the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "def grid_search_dbscan(data, options, verbose=False):\n",
    "\tresults = []\n",
    "\n",
    "\t# Iterate through all combinations of options\n",
    "\t# And perform a DBSCAN evaluation on each\n",
    "\toption_combs = itertools.product(*options.values())\n",
    "\n",
    "\tfor eps, min_samples in list(option_combs):\n",
    "\t\tif verbose: print(f'Running eps={eps}, min_samples={min_samples}')\n",
    "\n",
    "\t\tdb = DBSCAN(eps=eps, min_samples=min_samples).fit(data)\n",
    "\t\tlabels = db.labels_\n",
    "\n",
    "\t\t# Get estimated number of clusters and noise points\n",
    "\t\tclusters = num_of_clusters(db)\n",
    "\t\tnoise = list(labels).count(-1)\n",
    "\n",
    "\t\t# Silhouette Coefficient\n",
    "\t\tsilhouette = metrics.silhouette_score(X, labels)\n",
    "\n",
    "\t\t# Davies-Bouldin Score\n",
    "\t\tdavies_bouldin = metrics.davies_bouldin_score(X, labels)\n",
    "\n",
    "\t\tif verbose: print(f'Finished with silhouette={silhouette}, davies={davies_bouldin}')\n",
    "\n",
    "\t\trow = {\n",
    "\t\t\t'eps': eps,\n",
    "\t\t\t'min_samples': min_samples,\n",
    "\t\t\t'clusters': clusters,\n",
    "\t\t\t'noise': noise,\n",
    "\t\t\t'silhouette': silhouette,\n",
    "\t\t\t'davies_bouldin': davies_bouldin\n",
    "\t\t}\n",
    "\n",
    "\t\tresults.append(row)\n",
    "\n",
    "\treturn results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "DBSCAN parameters based on scikit learn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).\n",
    "\n",
    "Unsupervised clustering metrics based on scikit learn [documentation](https://scikit-learn.org/stable/modules/classes.html#clustering-metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eps=0.13, min_samples=8\n",
      "Finished with silhouette=-0.5886320830179866, davies=1.2538953420747236\n",
      "Running eps=0.13, min_samples=9\n",
      "Finished with silhouette=-0.5856643896916622, davies=1.3725254232080082\n",
      "Running eps=0.13, min_samples=10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Run grid search with the following options\u001b[39;00m\n\u001b[0;32m      2\u001b[0m options \u001b[39m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m \t\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m0.13\u001b[39m, \u001b[39m0.14\u001b[39m, \u001b[39m0.15\u001b[39m, \u001b[39m0.16\u001b[39m, \u001b[39m0.17\u001b[39m],\n\u001b[0;32m      4\u001b[0m \t\u001b[39m\"\u001b[39m\u001b[39mmin_samples\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m8\u001b[39m, \u001b[39m9\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m11\u001b[39m, \u001b[39m12\u001b[39m]\n\u001b[0;32m      5\u001b[0m }\n\u001b[1;32m----> 7\u001b[0m results \u001b[39m=\u001b[39m grid_search_dbscan(X, options, \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m, in \u001b[0;36mgrid_search_dbscan\u001b[1;34m(data, options, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m noise \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(labels)\u001b[39m.\u001b[39mcount(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Silhouette Coefficient\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m silhouette \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49msilhouette_score(X, labels)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Davies-Bouldin Score\u001b[39;00m\n\u001b[0;32m     26\u001b[0m davies_bouldin \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mdavies_bouldin_score(X, labels)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:117\u001b[0m, in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39;49mmetric, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:237\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    233\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[0;32m    234\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[0;32m    235\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[0;32m    236\u001b[0m )\n\u001b[1;32m--> 237\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mpairwise_distances_chunked(X, reduce_func\u001b[39m=\u001b[39;49mreduce_func, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[0;32m    238\u001b[0m intra_clust_dists, inter_clust_dists \u001b[39m=\u001b[39m results\n\u001b[0;32m    239\u001b[0m intra_clust_dists \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(intra_clust_dists)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1867\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1866\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[1;32m-> 1867\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m   1868\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[0;32m   1869\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   1871\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[0;32m   1872\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[0;32m   1873\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2039\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m   2037\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m-> 2039\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1579\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1576\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1578\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 1579\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m   1581\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1582\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:328\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m Y_norm_squared\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m1\u001b[39m, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m    323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible dimensions for Y of shape \u001b[39m\u001b[39m{\u001b[39;00mY\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mY_norm_squared of shape \u001b[39m\u001b[39m{\u001b[39;00moriginal_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         )\n\u001b[1;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:369\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    366\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[0;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    370\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[0;32m    371\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:192\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39m b\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m--> 192\u001b[0m     sparse\u001b[39m.\u001b[39;49missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\sparse\\_base.py:1301\u001b[0m, in \u001b[0;36misspmatrix\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1297\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1298\u001b[0m             \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n\u001b[1;32m-> 1301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misspmatrix\u001b[39m(x):\n\u001b[0;32m   1302\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Is x of a sparse matrix type?\u001b[39;00m\n\u001b[0;32m   1303\u001b[0m \n\u001b[0;32m   1304\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(x, spmatrix)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run grid search with the following options\n",
    "options = {\n",
    "\t\"eps\": [0.13, 0.14, 0.15, 0.16, 0.17],\n",
    "\t\"min_samples\": [8, 9, 10, 11, 12]\n",
    "}\n",
    "\n",
    "results = grid_search_dbscan(X, options, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eps': 0.13,\n",
       " 'min_samples': 8,\n",
       " 'clusters': 9,\n",
       " 'noise': 10532,\n",
       " 'silhouette': -0.5886320830179866,\n",
       " 'davies_bouldin': 1.2538953420747236}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the best parameters to perform DBSCAN and apply the labels to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=best['params']['eps'], min_samples=best['params']['min_samples']).fit(X)\n",
    "X_labeled = X.copy(deep=True)\n",
    "X_labeled['Label'] = db.labels_\n",
    "X_labeled.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then filter out rows with a label of -1 to denoise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_denoised = X_labeled[X_labeled['Label'] != -1]\n",
    "print(f'{X_labeled.shape[0] - X_denoised.shape[0]} points removed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "The following code was obtained from [scikit learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_clusters(db):\n",
    "\tunique_labels = set(db.labels_)\n",
    "\tcore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "\tcore_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "\tcolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "\tfor k, color in zip(unique_labels, colors):\n",
    "\t\tif k == -1:\n",
    "\t\t\t# Black used for noise\n",
    "\t\t\tcolor = [0, 0, 0, 1]\n",
    "\n",
    "\t\tclass_member_mask = db.labels_ == k\n",
    "\n",
    "\t\tcore_samples = X[class_member_mask & core_samples_mask]\n",
    "\n",
    "\t\tplt.plot(\n",
    "\t\t\tcore_samples['Easting'],\n",
    "\t\t\tcore_samples['Northing'],\n",
    "\t\t\t\"o\",\n",
    "\t\t\tmarkerfacecolor=tuple(color),\n",
    "\t\t\tmarkeredgecolor='k',\n",
    "\t\t\tmarkersize=10,\n",
    "\t\t)\n",
    "\n",
    "\t\tnon_core_samples = X[class_member_mask & ~core_samples_mask]\n",
    "\t\tplt.plot(\n",
    "\t\t\tnon_core_samples['Easting'],\n",
    "\t\t\tnon_core_samples['Northing'],\n",
    "\t\t\t\"o\",\n",
    "\t\t\tmarkerfacecolor=tuple(color),\n",
    "\t\t\tmarkeredgecolor='k',\n",
    "\t\t\tmarkersize=3,\n",
    "\t\t)\n",
    "\n",
    "\tplt.title(f'Estimated clusters: {num_of_clusters(db)}')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After denoised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2d(X_denoised)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Data\n",
    "\n",
    "Finally, we can export the denoised data to CSV format to be used by the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_denoised.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-add missing columns to keep Api code happy\n",
    "X_denoised.assign(Date='20-02-27')\n",
    "X_denoised.assign(Time='20:50:47.502')\n",
    "X_denoised.assign(Roll=-6.3)\n",
    "X_denoised.assign(Pitch=2.0)\n",
    "X_denoised.assign(Heading=19.9)\n",
    "\n",
    "X_denoised.to_csv('../Api/src/data/telemetry_denoised.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
